{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286a034b-dc8b-4403-b0e0-3020bd193e99",
   "metadata": {},
   "source": [
    "# Reproduction of the paper's results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bb533-d33f-4bc2-a3de-2b1e066f3751",
   "metadata": {},
   "source": [
    "## setup virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fce48-ec5b-4e06-89c6-436ca8efcbc2",
   "metadata": {},
   "source": [
    "install python version manager:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90d06f24-837f-4ba8-aa1f-ecfa46e4d492",
   "metadata": {},
   "source": [
    "curl https://pyenv.run | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c30b89-2bfa-4935-8652-4affeca75eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 -m venv myenv\n",
    "source myenv/bin/activate\n",
    "pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c738a-5d4c-4eb4-baa6-1ba922a460d4",
   "metadata": {},
   "source": [
    "Afterwards, restart JupyterLab and select \"myenv\" as kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1124f1-d631-46e3-9d9e-1c8e251b4339",
   "metadata": {},
   "source": [
    "## Install correct libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884ccb31-611e-49d3-8ebd-79e87e7ce02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/danielmazanek/myenv/lib/python3.11/site-packages (23.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/danielmazanek/myenv/lib/python3.11/site-packages (69.0.3)\n",
      "Requirement already satisfied: wheel in /Users/danielmazanek/myenv/lib/python3.11/site-packages (0.42.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping pandas as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping cython as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip uninstall --yes transformers\n",
    "%pip uninstall --yes torch\n",
    "%pip uninstall --yes pandas\n",
    "%pip uninstall --yes numpy\n",
    "%pip uninstall --yes scikit-learn\n",
    "%pip uninstall --yes cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236a7df5-3ab2-4048-a87b-440104685ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cython\n",
      "  Using cached Cython-3.0.7-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Using cached Cython-3.0.7-py2.py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: cython\n",
      "Successfully installed cython-3.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.19.2 (from versions: 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0rc1, 1.24.0rc2, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0rc1, 1.25.0, 1.25.1, 1.25.2, 1.26.0b1, 1.26.0rc1, 1.26.0, 1.26.1, 1.26.2, 1.26.3)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.19.2\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install transformers==3.0.2\n",
    "#%pip install torch==1.5.1\n",
    "#%pip install pandas==1.0.5\n",
    "%pip install --upgrade cython\n",
    "%pip install numpy==1.19.2 --only-binary :all:\n",
    "#%pip install scikit-learn==0.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc1fc1-bd41-4329-80a5-6a322364985a",
   "metadata": {},
   "source": [
    "## Author's code for predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97425928-be5a-4f78-a8c5-13fdff1af138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Constant variables \n",
    "class_names = ['Female', 'Male' , 'Neutral']\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "TEST_BATCH_SIZE = 16\n",
    "MAX_LEN = 55\n",
    "\n",
    "# Dataset\n",
    "class GenderBiasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, queries, tokenizer, max_len):\n",
    "        self.queries = queries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query_text = str(self.queries[index])\n",
    "         \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            query_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "                'query': query_text,\n",
    "                'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Dataloader\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GenderBiasDataset(\n",
    "    queries = df['query'].to_numpy(),\n",
    "    tokenizer  =tokenizer,\n",
    "    max_len = max_len\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 5 \n",
    "  )\n",
    "\n",
    "#Prediction function\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  query_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      texts = d[\"query\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs[0], dim=1)\n",
    "      query_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs[0])\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  return query_texts, predictions, prediction_probs\n",
    "\n",
    "\n",
    "#Reading MSMarco dev set queries (these queires do not have label)\n",
    "df = pd.read_csv(\"msmarco.csv\") # a dataframe containing the queries\n",
    "test_data_loader = create_data_loader(df, tokenizer, MAX_LEN, TEST_BATCH_SIZE)\n",
    "\n",
    "#Loading the fine-tuned model - you can download the model from https://drive.google.com/file/d/1_YTRs4v5DVUGUffnRHS_3Yk4qteJKO6w/view?usp=sharing\n",
    "print(\"Loading the Model\")\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 3)\n",
    "model.load_state_dict(torch.load(\"BERT_fine_tuned.bin\", map_location = device))\n",
    "print(\"Model Loaded Successfully\")\n",
    "\n",
    "print(\"Prediction started\")\n",
    "y_query_texts, y_pred, y_pred_probs = get_predictions(model, test_data_loader)\n",
    "prediction = pd.DataFrame(df.values.tolist(), columns = [\"qid\",\"query\"])\n",
    "prediction['female_probability'] = y_pred_probs[:, 0]\n",
    "prediction['male_probability'] = y_pred_probs[:, 1]\n",
    "prediction['neutral_probability'] = y_pred_probs[:, 2]\n",
    "prediction['prediction'] = y_pred\n",
    "prediction.to_csv(\"predictions.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ebbc7-05b3-4269-929b-4343e8dde241",
   "metadata": {},
   "source": [
    "## Author's code for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad03c97-e94c-49e3-8d33-3b036bcd9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "#Defining some key variables for preprocessing step\n",
    "class_names = ['Female', 'Male' , 'Neutral']\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN = 33\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "#Dataset\n",
    "class GenderBiasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, queries, targets, tokenizer, max_len):\n",
    "        self.queries = queries\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query_text = str(self.queries[index])\n",
    "        target = self.targets[index]\n",
    "         \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            query_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "                'query': query_text,\n",
    "                'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "                'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "#Dataloader\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GenderBiasDataset(\n",
    "    queries = df['query'].to_numpy(),\n",
    "    targets = df['label'].to_numpy(),\n",
    "    tokenizer  =tokenizer,\n",
    "    max_len = max_len\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 5\n",
    "  )\n",
    "\n",
    "#Training function\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      labels = targets\n",
    "    )\n",
    "    _, preds = torch.max(outputs[1], dim=1)  # the second return value is logits\n",
    "    loss = outputs[0] #the first return value is loss\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "#Evaluation function - used when adopting K-fold\n",
    "def eval_model(model, data_loader, device, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels = targets\n",
    "      )\n",
    "      _, preds = torch.max(outputs[1], dim=1)\n",
    "      loss = outputs[0]\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "#Prediction function - used to calculate the accuracy of the model when true labels are available\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  query_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      texts = d[\"query\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "\tlabels = targets\n",
    "      )\n",
    "      _, preds = torch.max(outputs[1], dim=1)\n",
    "      query_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs[1])\n",
    "      real_values.extend(targets)\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return query_texts, predictions, prediction_probs, real_values\n",
    "\n",
    "\n",
    "#Fine-Tuning the BERT on the Dataset\n",
    "result = open(\"BERT_Tuninig_results.txt\", \"w\")\n",
    "df = pd.read_csv(\"queries_gender_annotated.csv\", names = [\"query\", \"label\"]) \n",
    "labelEncoder = LabelEncoder()\n",
    "df['label'] = labelEncoder.fit_transform(df['label'])\n",
    "result.write(\"Shape of Dataset: {} \\n\".format(df.shape))\n",
    "wordlist = pd.read_csv(\"gender_specific_wordlist.csv\")\n",
    "wordlist['label'] = labelEncoder.fit_transform(wordlist['label'])\n",
    "df = pd.concat([df, wordlist], ignore_index = False)\n",
    "result.write(\"Shape of Dataset after concatination with wordlist: {} \\n\".format(df.shape))\n",
    "\n",
    "train_data_loader = create_data_loader(df, tokenizer, MAX_LEN, TRAIN_BATCH_SIZE)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 3) \n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(params =  model.parameters(), lr = LEARNING_RATE, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps = 0,\n",
    "            num_training_steps = total_steps\n",
    "        )\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    result.write(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    result.write(\"\\n\")\n",
    "    result.write('-' * 10)\n",
    "    result.write(\"\\n\")\n",
    "    train_acc, train_loss = train_epoch(\n",
    "                model,\n",
    "                train_data_loader,\n",
    "                optimizer,\n",
    "                device,\n",
    "                scheduler,\n",
    "                len(df)\n",
    "        )\n",
    "    result.write(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    result.write(\"\\n\")\n",
    "\n",
    "torch.save(model.state_dict(), \"BERT_fine_tuned.bin\")\n",
    "result.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
