{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286a034b-dc8b-4403-b0e0-3020bd193e99",
   "metadata": {},
   "source": [
    "# Reproduction of the paper's results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bb533-d33f-4bc2-a3de-2b1e066f3751",
   "metadata": {},
   "source": [
    "## setup virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fce48-ec5b-4e06-89c6-436ca8efcbc2",
   "metadata": {},
   "source": [
    "1. open new terminal and install python version manager:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90d06f24-837f-4ba8-aa1f-ecfa46e4d492",
   "metadata": {},
   "source": [
    "curl https://pyenv.run | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb40fa-5f7e-4d55-8390-0235fec97699",
   "metadata": {},
   "source": [
    "2. download old python version from web: https://www.python.org/downloads/release/python-375rc1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8b246-b643-4d01-b130-f6b93fe2ad08",
   "metadata": {},
   "source": [
    "3. also in Terminal, run these 3 commands to setup virtual environment:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ded75ca-f15f-4f36-98d5-e06f3a787739",
   "metadata": {},
   "source": [
    "python3.7 -m venv myenv-python37\n",
    "source myenv-python37/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=myenv-python37 --display-name=\"Python 3.7 (myenv-python37)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c738a-5d4c-4eb4-baa6-1ba922a460d4",
   "metadata": {},
   "source": [
    "4. Afterwards, restart JupyterLab and select \"Python 3.7 (myenv-python37)\" as kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1124f1-d631-46e3-9d9e-1c8e251b4339",
   "metadata": {},
   "source": [
    "## Install correct libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884ccb31-611e-49d3-8ebd-79e87e7ce02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (23.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (68.0.0)\n",
      "Requirement already satisfied: wheel in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (0.42.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: pandas 1.0.5\n",
      "Uninstalling pandas-1.0.5:\n",
      "  Successfully uninstalled pandas-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: numpy 1.19.2\n",
      "Uninstalling numpy-1.19.2:\n",
      "  Successfully uninstalled numpy-1.19.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: scikit-learn 0.23.1\n",
      "Uninstalling scikit-learn-0.23.1:\n",
      "  Successfully uninstalled scikit-learn-0.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping cython as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip uninstall --yes transformers\n",
    "%pip uninstall --yes torch\n",
    "%pip uninstall --yes pandas\n",
    "%pip uninstall --yes numpy\n",
    "%pip uninstall --yes scikit-learn\n",
    "%pip uninstall --yes cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236a7df5-3ab2-4048-a87b-440104685ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.0.2\n",
      "  Using cached transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "Collecting numpy (from transformers==3.0.2)\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-macosx_10_9_x86_64.whl (16.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers==0.8.1.rc1 (from transformers==3.0.2)\n",
      "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-macosx_10_10_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from transformers==3.0.2) (23.2)\n",
      "Collecting filelock (from transformers==3.0.2)\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting requests (from transformers==3.0.2)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==3.0.2)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==3.0.2)\n",
      "  Downloading regex-2023.12.25-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92 (from transformers==3.0.2)\n",
      "  Downloading sentencepiece-0.1.99-cp37-cp37m-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sacremoses (from transformers==3.0.2)\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests->transformers==3.0.2)\n",
      "  Downloading charset_normalizer-3.3.2-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers==3.0.2)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==3.0.2)\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers==3.0.2)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (1.16.0)\n",
      "Collecting click (from sacremoses->transformers==3.0.2)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (1.3.2)\n",
      "Collecting importlib-metadata (from click->sacremoses->transformers==3.0.2)\n",
      "  Downloading importlib_metadata-6.7.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata->click->sacremoses->transformers==3.0.2)\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting typing-extensions>=3.6.4 (from importlib-metadata->click->sacremoses->transformers==3.0.2)\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Downloading regex-2023.12.25-cp37-cp37m-macosx_10_9_x86_64.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp37-cp37m-macosx_10_9_x86_64.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895239 sha256=7fdf9c95163b5cbe38ad66f0c1c1e5d2f2fef4d5a9d8f9f358cddadf6412e78e\n",
      "  Stored in directory: /Users/danielmazanek/Library/Caches/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, zipp, urllib3, typing-extensions, tqdm, regex, numpy, idna, filelock, charset-normalizer, certifi, requests, importlib-metadata, click, sacremoses, transformers\n",
      "Successfully installed certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 filelock-3.12.2 idna-3.6 importlib-metadata-6.7.0 numpy-1.21.6 regex-2023.12.25 requests-2.31.0 sacremoses-0.0.53 sentencepiece-0.1.99 tokenizers-0.8.1rc1 tqdm-4.66.1 transformers-3.0.2 typing-extensions-4.7.1 urllib3-2.0.7 zipp-3.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch==1.5.1\n",
      "  Downloading torch-1.5.1-cp37-none-macosx_10_9_x86_64.whl (80.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.5/80.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting future (from torch==1.5.1)\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from torch==1.5.1) (1.21.6)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=b02e4f1d87e7fad670cbd005eac98fe83f17a096cd942eef727b8ec4d96b2b55\n",
      "  Stored in directory: /Users/danielmazanek/Library/Caches/pip/wheels/fa/cd/1f/c6b7b50b564983bf3011e8fc75d06047ddc50c07f6e3660b00\n",
      "Successfully built future\n",
      "Installing collected packages: future, torch\n",
      "Successfully installed future-0.18.3 torch-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas==1.0.5\n",
      "  Using cached pandas-1.0.5-cp37-cp37m-macosx_10_9_x86_64.whl (10.0 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from pandas==1.0.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from pandas==1.0.5) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from pandas==1.0.5) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy==1.19.2\n",
      "  Using cached numpy-1.19.2-cp37-cp37m-macosx_10_9_x86_64.whl (15.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "Successfully installed numpy-1.19.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn==0.23.1\n",
      "  Using cached scikit_learn-0.23.1-cp37-cp37m-macosx_10_9_x86_64.whl (7.2 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from scikit-learn==0.23.1) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from scikit-learn==0.23.1) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from scikit-learn==0.23.1) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/danielmazanek/myenv-python37/lib/python3.7/site-packages (from scikit-learn==0.23.1) (3.1.0)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-0.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==3.0.2\n",
    "%pip install torch==1.5.1\n",
    "%pip install pandas==1.0.5\n",
    "#%pip install Cython==0.29.21\n",
    "%pip install numpy==1.19.2\n",
    "%pip install scikit-learn==0.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc1fc1-bd41-4329-80a5-6a322364985a",
   "metadata": {},
   "source": [
    "## Author's code for predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97425928-be5a-4f78-a8c5-13fdff1af138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmazanek/myenv-python37/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|████████████████████████████| 232k/232k [00:01<00:00, 122kB/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File msmarco.csv does not exist: 'msmarco.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5c/r0r98wnj7xx1b0tt595pdz100000gn/T/ipykernel_18466/2229246679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m#Reading MSMarco dev set queries (these queires do not have label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"msmarco.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# a dataframe containing the queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mtest_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv-python37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv-python37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv-python37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv-python37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv-python37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File msmarco.csv does not exist: 'msmarco.csv'"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Constant variables \n",
    "class_names = ['Female', 'Male' , 'Neutral']\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "TEST_BATCH_SIZE = 16\n",
    "MAX_LEN = 55\n",
    "\n",
    "# Dataset\n",
    "class GenderBiasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, queries, tokenizer, max_len):\n",
    "        self.queries = queries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query_text = str(self.queries[index])\n",
    "         \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            query_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "                'query': query_text,\n",
    "                'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Dataloader\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GenderBiasDataset(\n",
    "    queries = df['query'].to_numpy(),\n",
    "    tokenizer  =tokenizer,\n",
    "    max_len = max_len\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 5 \n",
    "  )\n",
    "\n",
    "#Prediction function\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  query_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      texts = d[\"query\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs[0], dim=1)\n",
    "      query_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs[0])\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  return query_texts, predictions, prediction_probs\n",
    "\n",
    "\n",
    "#Reading MSMarco dev set queries (these queires do not have label)\n",
    "df = pd.read_csv(\"msmarco.csv\") # a dataframe containing the queries\n",
    "test_data_loader = create_data_loader(df, tokenizer, MAX_LEN, TEST_BATCH_SIZE)\n",
    "\n",
    "#Loading the fine-tuned model - you can download the model from https://drive.google.com/file/d/1_YTRs4v5DVUGUffnRHS_3Yk4qteJKO6w/view?usp=sharing\n",
    "print(\"Loading the Model\")\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 3)\n",
    "model.load_state_dict(torch.load(\"BERT_fine_tuned.bin\", map_location = device))\n",
    "print(\"Model Loaded Successfully\")\n",
    "\n",
    "print(\"Prediction started\")\n",
    "y_query_texts, y_pred, y_pred_probs = get_predictions(model, test_data_loader)\n",
    "prediction = pd.DataFrame(df.values.tolist(), columns = [\"qid\",\"query\"])\n",
    "prediction['female_probability'] = y_pred_probs[:, 0]\n",
    "prediction['male_probability'] = y_pred_probs[:, 1]\n",
    "prediction['neutral_probability'] = y_pred_probs[:, 2]\n",
    "prediction['prediction'] = y_pred\n",
    "prediction.to_csv(\"predictions.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ebbc7-05b3-4269-929b-4343e8dde241",
   "metadata": {},
   "source": [
    "## Author's code for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad03c97-e94c-49e3-8d33-3b036bcd9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "#Defining some key variables for preprocessing step\n",
    "class_names = ['Female', 'Male' , 'Neutral']\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN = 33\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "#Dataset\n",
    "class GenderBiasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, queries, targets, tokenizer, max_len):\n",
    "        self.queries = queries\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query_text = str(self.queries[index])\n",
    "        target = self.targets[index]\n",
    "         \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            query_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "                'query': query_text,\n",
    "                'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "                'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "#Dataloader\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GenderBiasDataset(\n",
    "    queries = df['query'].to_numpy(),\n",
    "    targets = df['label'].to_numpy(),\n",
    "    tokenizer  =tokenizer,\n",
    "    max_len = max_len\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 5\n",
    "  )\n",
    "\n",
    "#Training function\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      labels = targets\n",
    "    )\n",
    "    _, preds = torch.max(outputs[1], dim=1)  # the second return value is logits\n",
    "    loss = outputs[0] #the first return value is loss\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "#Evaluation function - used when adopting K-fold\n",
    "def eval_model(model, data_loader, device, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels = targets\n",
    "      )\n",
    "      _, preds = torch.max(outputs[1], dim=1)\n",
    "      loss = outputs[0]\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "#Prediction function - used to calculate the accuracy of the model when true labels are available\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  query_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      texts = d[\"query\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "\tlabels = targets\n",
    "      )\n",
    "      _, preds = torch.max(outputs[1], dim=1)\n",
    "      query_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs[1])\n",
    "      real_values.extend(targets)\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return query_texts, predictions, prediction_probs, real_values\n",
    "\n",
    "\n",
    "#Fine-Tuning the BERT on the Dataset\n",
    "result = open(\"BERT_Tuninig_results.txt\", \"w\")\n",
    "df = pd.read_csv(\"queries_gender_annotated.csv\", names = [\"query\", \"label\"]) \n",
    "labelEncoder = LabelEncoder()\n",
    "df['label'] = labelEncoder.fit_transform(df['label'])\n",
    "result.write(\"Shape of Dataset: {} \\n\".format(df.shape))\n",
    "wordlist = pd.read_csv(\"gender_specific_wordlist.csv\")\n",
    "wordlist['label'] = labelEncoder.fit_transform(wordlist['label'])\n",
    "df = pd.concat([df, wordlist], ignore_index = False)\n",
    "result.write(\"Shape of Dataset after concatination with wordlist: {} \\n\".format(df.shape))\n",
    "\n",
    "train_data_loader = create_data_loader(df, tokenizer, MAX_LEN, TRAIN_BATCH_SIZE)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 3) \n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(params =  model.parameters(), lr = LEARNING_RATE, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps = 0,\n",
    "            num_training_steps = total_steps\n",
    "        )\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    result.write(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    result.write(\"\\n\")\n",
    "    result.write('-' * 10)\n",
    "    result.write(\"\\n\")\n",
    "    train_acc, train_loss = train_epoch(\n",
    "                model,\n",
    "                train_data_loader,\n",
    "                optimizer,\n",
    "                device,\n",
    "                scheduler,\n",
    "                len(df)\n",
    "        )\n",
    "    result.write(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    result.write(\"\\n\")\n",
    "\n",
    "torch.save(model.state_dict(), \"BERT_fine_tuned.bin\")\n",
    "result.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (myenv-python37)",
   "language": "python",
   "name": "myenv-python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
