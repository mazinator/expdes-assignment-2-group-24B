{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218f864d-0419-4a1e-a67d-183e4f64c35f",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91c698bf-e27b-4c7b-b0b6-806118414e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Female', 'Male' , 'Neutral']\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN = 33\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# importing libraries\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Constant variables \n",
    "class_names = ['Female', 'Male' , 'Neutral']\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "TEST_BATCH_SIZE = 16\n",
    "MAX_LEN = 55\n",
    "\n",
    "class GenderBiasDataset(Dataset):\n",
    "    def __init__(self, queries, targets, tokenizer, max_len):\n",
    "        self.queries = queries\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query_text = str(self.queries[index])\n",
    "        target = self.targets[index]\n",
    "         \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            query_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "                'query': query_text,\n",
    "                'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "                'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Dataloader\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GenderBiasDataset(\n",
    "    queries = df['query'].to_numpy(),\n",
    "    targets = df['label'].to_numpy(),\n",
    "    tokenizer  =tokenizer,\n",
    "    max_len = max_len\n",
    "  )\n",
    "    return DataLoader(\n",
    "    ds,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 5\n",
    "  )\n",
    "\n",
    "#Training function\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      labels = targets\n",
    "    )\n",
    "    _, preds = torch.max(outputs[1], dim=1)  # the second return value is logits\n",
    "    loss = outputs[0] #the first return value is loss\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "\n",
    "#Evaluation function - used when adopting K-fold\n",
    "def eval_model(model, data_loader, device, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels = targets\n",
    "      )\n",
    "      _, preds = torch.max(outputs[1], dim=1)\n",
    "      loss = outputs[0]\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "#Prediction function - used to calculate the accuracy of the model when true labels are available\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  query_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      texts = d[\"query\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "\tlabels = targets\n",
    "      )\n",
    "      _, preds = torch.max(outputs[1], dim=1)\n",
    "      query_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs[1])\n",
    "      real_values.extend(targets)\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return query_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e055294-8b93-432a-abe4-e4e36ad37b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  label\n",
      "0                  who was known as the heretic king      1\n",
      "1  who plays the main character in night at the m...      2\n",
      "2                                  what is surrogate      2\n",
      "3                      how popular is the name katie      0\n",
      "4         how much sleep in one day does a baby need      2\n",
      "5         what type of books does karen hesse write?      0\n",
      "6            can you drink coffee before a mammogram      2\n",
      "7                  what college did bill gates go to      1\n",
      "8      who was jacqueline kennedy's social secretary      0\n",
      "9                                   abbot definition      2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1/5\n",
      "epoch 1/8\n"
     ]
    }
   ],
   "source": [
    "res = open(\"CV_res.txt\", \"w\")\n",
    "df = pd.read_csv(\"./data/queries_gender_annotated.csv\", names = [\"index\", \"query\", \"label\", \"other\"])\n",
    "df['label'] = df['label'].astype(str)\n",
    "df = df.apply(process_row, axis=1)\n",
    "df.drop(columns=[\"index\", \"other\"], inplace=True)\n",
    "df = df[df['label'].isin(['m', 'n', 'f'])]\n",
    "labelEncoder = LabelEncoder()\n",
    "df['label'] = labelEncoder.fit_transform(df['label'])\n",
    "\n",
    "res.write(\"Shape of Dataset: {} \\n\".format(df.shape))\n",
    "wordlist = pd.read_csv(\"./data/wordlist_genderspecific.txt\", names = [\"query\", \"label\"])\n",
    "wordlist['label'] = labelEncoder.fit_transform(wordlist['label'])\n",
    "df = pd.concat([df, wordlist], ignore_index=False)\n",
    "res.write(\"Shape of Dataset after concatenation: {} \\n\".format(df.shape))\n",
    "num_folds = 5\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_folds = []\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(df['query'], df['label'])):\n",
    "    train_data_loader = create_data_loader(df.iloc[train_index], tokenizer, MAX_LEN, TRAIN_BATCH_SIZE)\n",
    "    val_data_loader = create_data_loader(df.iloc[val_index], tokenizer, MAX_LEN, VALID_BATCH_SIZE)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=3)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
    "    total_steps = len(train_data_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    res.write(f'fold {fold + 1}/{num_folds}')\n",
    "    print(f'fold {fold + 1}/{num_folds}')\n",
    "    res.write(\"\\n\")\n",
    "    res.write('-' * 10)\n",
    "    res.write(\"\\n\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        res.write(f'epoch {epoch + 1}/{EPOCHS}')\n",
    "        print(f'epoch {epoch + 1}/{EPOCHS}')\n",
    "        res.write(\"\\n\")\n",
    "        res.write('-' * 10)\n",
    "        res.write(\"\\n\")\n",
    "\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            model,\n",
    "            train_data_loader,\n",
    "            optimizer,\n",
    "            device,\n",
    "            scheduler,\n",
    "            len(df)\n",
    "        )\n",
    "        \n",
    "        val_acc, val_loss = eval_model(\n",
    "            model,\n",
    "            val_data_loader,\n",
    "            device,\n",
    "            len(df)/num_folds,\n",
    "        )\n",
    "        \n",
    "        res.write(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "        res.write(\"\\n\")\n",
    "        \n",
    "    accuracy_folds.append(val_acc)\n",
    "    \n",
    "res.write(f'mean accuracy over all folds: {sum(accuracy_folds)/len(accuracy_folds)}')\n",
    "res.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (myenv-python37)",
   "language": "python",
   "name": "myenv-python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
