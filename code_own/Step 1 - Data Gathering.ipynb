{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of file 'queries_gender_annotated.csv' needed, as separator is ',' and second column contains text which also contains ','. Hence, pandas cannot read the file without problems.\n",
    "Solution: put \"\" around the text\n",
    "download files first and put into subfolder 'code_own\\data'. Also, you need to unpack the compressed files directly into the data folder: \n",
    "- curl -O https://github.com/navid-rekabsaz/GenderBias_IR/blob/master/resources/queries_gender_annotated.csv\n",
    "- curl -O https://github.com/navid-rekabsaz/GenderBias_IR/blob/master/resources/wordlist_genderspecific.txt\n",
    "- curl -O https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz\n",
    "- curl -O https://msmarco.blob.core.windows.net/msmarcoranking/qrels.dev.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ANNOTATED_DATASET = 'data/queries_gender_annotated.csv'\n",
    "PATH_TARGET_MODIFIED_ANNOTATED_DATASET = 'data/queries_gender_annotated_modified.csv'\n",
    "\n",
    "PATH_QUERIES = 'data/queries.dev.tsv'\n",
    "PATH_QRELS = 'data/qrels.dev.tsv'\n",
    "PATH_TARGET_MSMACRO = 'data/msmacro.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_annotated_dataset(source, target):\n",
    "\n",
    "    #preprcess queries_gender_annotated.csv as it cannot be read witout errors (, in text)\n",
    "    #iterate over all rows of file queries_gender_annotated.csv\n",
    "    result_query_gender_annontated = []\n",
    "    with open(source, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            #split by comma\n",
    "            split = line.split(',')\n",
    "            #remove newline in last element\n",
    "            split[-1] = split[-1].replace('\\n', '')\n",
    "            #put into 3 columns (if ',' in text)\n",
    "            if len(split) > 3:\n",
    "                #combine all but first and last element\n",
    "                text = ','.join(split[1:-1])\n",
    "            else:\n",
    "                text = split[1]\n",
    "\n",
    "            #combine elements and put text in quotes\n",
    "            new_line = split[0] + ',\"' + text + '\",' + split[-1] + '\\n'\n",
    "            #print(new_line)\n",
    "            result_query_gender_annontated.append(new_line)\n",
    "\n",
    "    #add headers to file\n",
    "    result_query_gender_annontated.insert(0, 'qid,query,annotation\\n')\n",
    "\n",
    "    #write to file\n",
    "    with open(target, 'w') as f:\n",
    "        f.writelines(result_query_gender_annontated)\n",
    "\n",
    "transform_annotated_dataset(PATH_ANNOTATED_DATASET, PATH_TARGET_MODIFIED_ANNOTATED_DATASET)\n",
    "df_gender = pd.read_csv(PATH_TARGET_MODIFIED_ANNOTATED_DATASET, header=0, sep=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'msmacro.csv' isn't provided either. Therefore, we need to gather the data ourselves.\n",
    "As the MS Macro dataset is divided into multiple files, we will need to use multiple files too to obtain the final file.\n",
    "\n",
    "The authors of our paper described, that they used the *dev* sets, which had at least one related human-judged relevance judgement document. The human-judged relevance judgement documents are contained in the *qrels.dev.csv* file. \n",
    "\n",
    "On the source website if the qrels dataset it is described that the qrels file is in the *TREC qrels format* [https://github.com/microsoft/msmarco/blob/master/TREC-Deep-Learning-2019.md; https://microsoft.github.io/msmarco/].\n",
    "\n",
    "This format has 4 columns (TOPIC, ITERATION, DOCUMENT, RELEVANCY) [https://trec.nist.gov/data/qrels_eng/]. In our case the TOPIC equals the query(id), which is in out case with RELEVANCY the only relevant column for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Read the rqrels file and the queries file\n",
    "df_qrels = pd.read_csv(PATH_QRELS, sep='\\t', header=None)\n",
    "df_queries = pd.read_csv(PATH_QUERIES, sep='\\t', header=None)\n",
    "\n",
    "#set column names\n",
    "df_qrels.columns = ['qid', 'iter', 'docid', 'rel']\n",
    "df_queries.columns = ['qid', 'query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in qid:  3695\n",
      "Number of irrelevant rows:  0\n"
     ]
    }
   ],
   "source": [
    "#check for duplicates in column qid\n",
    "print(\"Duplicates in qid: \", df_qrels.duplicated('qid').sum())\n",
    "\n",
    "\n",
    "#check for irrelevant rows\n",
    "print(\"Number of irrelevant rows: \", df_qrels[df_qrels['rel'] != 1]['rel'].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have duplicated rows, but all rows are relevant. Therefore, we will remove the duplicates and also the queries, which are contained in the *annotated gender dataset*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant queries:  51828\n"
     ]
    }
   ],
   "source": [
    "#remove duplicates\n",
    "relevant_qid = set(df_qrels['qid'].unique())\n",
    "\n",
    "\n",
    "#remove qids that are in df_gender\n",
    "relevant_qid = relevant_qid - set(df_gender['qid'].unique())\n",
    "\n",
    "#select only queries with relevant qid\n",
    "df_queries_relevant = df_queries[df_queries['qid'].isin(relevant_qid)]\n",
    "\n",
    "print(\"Number of relevant queries: \", df_queries_relevant['qid'].count())\n",
    "\n",
    "df_queries_relevant.to_csv(PATH_TARGET_MSMACRO, index=False, header=True, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get **51828** relevant queries. In our assigned paper however, there are 51,827 relevant queries (1 less). We cannot explain the single row difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
