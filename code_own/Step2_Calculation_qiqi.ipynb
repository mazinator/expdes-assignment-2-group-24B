{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286a034b-dc8b-4403-b0e0-3020bd193e99",
   "metadata": {},
   "source": [
    "# Reproduction of the paper's results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bb533-d33f-4bc2-a3de-2b1e066f3751",
   "metadata": {},
   "source": [
    "## setup virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fce48-ec5b-4e06-89c6-436ca8efcbc2",
   "metadata": {},
   "source": [
    "1. open new terminal and install python version manager:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90d06f24-837f-4ba8-aa1f-ecfa46e4d492",
   "metadata": {},
   "source": [
    "curl https://pyenv.run | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb40fa-5f7e-4d55-8390-0235fec97699",
   "metadata": {},
   "source": [
    "2. download old python version from web: https://www.python.org/downloads/release/python-375rc1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8b246-b643-4d01-b130-f6b93fe2ad08",
   "metadata": {},
   "source": [
    "3. also in Terminal, run these 4 commands to setup virtual environment:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ded75ca-f15f-4f36-98d5-e06f3a787739",
   "metadata": {},
   "source": [
    "python3.7 -m venv myenv-python37\n",
    "source myenv-python37/bin/activate (for Windows: myenv\\Scripts\\activate)\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=myenv-python37 --display-name=\"Python 3.7 (myenv-python37)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c738a-5d4c-4eb4-baa6-1ba922a460d4",
   "metadata": {},
   "source": [
    "4. Afterwards, restart JupyterLab and select \"Python 3.7 (myenv-python37)\" as kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1124f1-d631-46e3-9d9e-1c8e251b4339",
   "metadata": {},
   "source": [
    "## Uninstall libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884ccb31-611e-49d3-8ebd-79e87e7ce02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 3.0.2\n",
      "Uninstalling transformers-3.0.2:\n",
      "  Successfully uninstalled transformers-3.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: torch 1.5.1\n",
      "Uninstalling torch-1.5.1:\n",
      "  Successfully uninstalled torch-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: pandas 1.0.5\n",
      "Uninstalling pandas-1.0.5:\n",
      "  Successfully uninstalled pandas-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: numpy 1.19.2\n",
      "Uninstalling numpy-1.19.2:\n",
      "  Successfully uninstalled numpy-1.19.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: scikit-learn 0.23.1\n",
      "Uninstalling scikit-learn-0.23.1:\n",
      "  Successfully uninstalled scikit-learn-0.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping cython as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall --yes transformers\n",
    "%pip uninstall --yes torch\n",
    "%pip uninstall --yes pandas\n",
    "%pip uninstall --yes numpy\n",
    "%pip uninstall --yes scikit-learn\n",
    "%pip uninstall --yes cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43cd38-6ab4-44bd-a815-9d5408c15923",
   "metadata": {},
   "source": [
    "## Install libraries with software versions stated in \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a7df5-3ab2-4048-a87b-440104685ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip install transformers==3.0.2\n",
    "%pip install torch==1.5.1\n",
    "%pip install pandas==1.0.5\n",
    "%pip install numpy==1.19.2\n",
    "%pip install scikit-learn==0.23.1\n",
    "%pip install --upgrade jupyter ipywidgets\n",
    "%jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16539c7e-d3e8-4ea7-9ce4-50afb816812b",
   "metadata": {},
   "source": [
    "## Functions to be added by us before running author's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8688978f-4b0a-4faf-8956-96b6961f9a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this function is needed because the delimiter \",\" is also apparent in some queries\n",
    "def process_row(row):\n",
    "    if row['label'] not in ['m', 'n', 'f']:\n",
    "        row[\"query\"] = row[\"query\"] + row[\"label\"]\n",
    "        row[\"label\"] = row[\"other\"]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ad905-89a4-4932-a3cb-8b75910a1382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade ipython jupyter notebook torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ebbc7-05b3-4269-929b-4343e8dde241",
   "metadata": {},
   "source": [
    "## Author's code for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ad03c97-e94c-49e3-8d33-3b036bcd9adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "#Defining some key variables for preprocessing step\n",
    "class_names = ['Female', 'Male' , 'Neutral']\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN = 33\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "#Dataset\n",
    "class GenderBiasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, queries, targets, tokenizer, max_len):\n",
    "        self.queries = queries\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query_text = str(self.queries[index])\n",
    "        target = self.targets[index]\n",
    "         \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            query_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "                'query': query_text,\n",
    "                'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "                'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "#Dataloader\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GenderBiasDataset(\n",
    "        queries = df['query'].to_numpy(),\n",
    "        targets = df['label'].to_numpy(),\n",
    "        tokenizer  =tokenizer,\n",
    "        max_len = max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 5\n",
    "    )\n",
    "\n",
    "#Training function\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels = targets\n",
    "        )\n",
    "        _, preds = torch.max(outputs[1], dim=1)  # the second return value is logits\n",
    "        loss = outputs[0] #the first return value is loss\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "#Evaluation function - used when adopting K-fold\n",
    "def eval_model(model, data_loader, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels = targets\n",
    "            )\n",
    "        _, preds = torch.max(outputs[1], dim=1)\n",
    "        loss = outputs[0]\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "#Prediction function - used to calculate the accuracy of the model when true labels are available\n",
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    query_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"query\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            \tlabels = targets\n",
    "              )\n",
    "        _, preds = torch.max(outputs[1], dim=1)\n",
    "        query_texts.extend(texts)\n",
    "        predictions.extend(preds)\n",
    "        prediction_probs.extend(outputs[1])\n",
    "        real_values.extend(targets)\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return query_texts, predictions, prediction_probs, real_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fed0b-8ccc-4637-845d-0bbb9e499bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#--Adopting 5-fold cross-validation---------------------------------------------------------#\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "\n",
    "result = open(\"BERT_Tuninig_results_accuracy.txt\", \"w\")\n",
    "\n",
    "df = pd.read_csv(\"./data/queries_gender_annotated.csv\", names = [\"index\", \"query\", \"label\", \"other\"]) \n",
    "df['label'] = df['label'].astype(str)\n",
    "df = df.apply(process_row, axis=1)\n",
    "df.drop(columns=[\"index\", \"other\"], inplace=True)\n",
    "df = df[df['label'].isin(['m', 'n', 'f'])]\n",
    "labelEncoder = LabelEncoder()\n",
    "df['label'] = labelEncoder.fit_transform(df['label'])\n",
    "wordlist = pd.read_csv(\"./data/wordlist_genderspecific.txt\", names = [\"query\", \"label\"])\n",
    "wordlist['label'] = labelEncoder.fit_transform(wordlist['label'])\n",
    "df = pd.concat([df, wordlist], ignore_index=True)\n",
    "\n",
    "# StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# 5 fold cross-validation \n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df.index, df['label'])):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    result.write(f'Fold {fold + 1}\\n')\n",
    "\n",
    "    # Divide the training set and validation set\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    # Creating a Data Loader\n",
    "    train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, TRAIN_BATCH_SIZE)\n",
    "    val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, VALID_BATCH_SIZE)\n",
    "\n",
    "    # Initialising the BERT model\n",
    "    model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=3)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optimiser and Learning Plan\n",
    "    optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
    "    total_steps = len(train_data_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training model\n",
    "    for epoch in range(EPOCHS):\n",
    "        result.write(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "        result.write(\"\\n\")\n",
    "        result.write('-' * 10)\n",
    "        result.write(\"\\n\")\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            model,\n",
    "            train_data_loader,\n",
    "            optimizer,\n",
    "            device,\n",
    "            scheduler,\n",
    "            len(train_df)\n",
    "        )\n",
    "        result.write(f'Epoch {epoch + 1}/{EPOCHS} | Train loss: {train_loss} | Train accuracy: {train_acc}')\n",
    "        result.write(\"\\n\")\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS} | Train loss: {train_loss} | Train accuracy: {train_acc}')\n",
    "   \n",
    "    # validing model\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        device,\n",
    "        len(val_df)\n",
    "    )\n",
    "\n",
    "    # Store the accuracy of each fold\n",
    "    accuracies.append(val_acc)\n",
    "    result.write(f'Validation accuracy for fold {fold}: {val_acc}\\n')\n",
    "    print(f'Validation accuracy for fold {fold}: {val_acc}')\n",
    "     \n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        torch.save(model.state_dict(), \"BERT_fine_tuned_5fold.bin\")\n",
    "\n",
    "# calc the avarage accuracy \n",
    "average_accuracy = np.mean(accuracies)\n",
    "result.write(f'Average Accuracy across folds: {average_accuracy}\\n')\n",
    "print(f'Average Accuracy across folds: {average_accuracy}')\n",
    "\n",
    "\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a40087c9-577e-4a44-bf90-b5bbe1169f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Train loss: 0.5719224624927082 | Train accuracy: 0.7834880636604774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.0026490066225165563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#=========================== simple dataset split=================================#\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "result = open(\"BERT_Tuninig_results_accuracy.txt\", \"w\")\n",
    "\n",
    "df = pd.read_csv(\"./data/queries_gender_annotated.csv\", names = [\"index\", \"query\", \"label\", \"other\"]) \n",
    "df['label'] = df['label'].astype(str)\n",
    "df = df.apply(process_row, axis=1)\n",
    "df.drop(columns=[\"index\", \"other\"], inplace=True)\n",
    "df = df[df['label'].isin(['m', 'n', 'f'])]\n",
    "labelEncoder = LabelEncoder()\n",
    "df['label'] = labelEncoder.fit_transform(df['label'])\n",
    "wordlist = pd.read_csv(\"./data/wordlist_genderspecific.txt\", names = [\"query\", \"label\"])\n",
    "wordlist['label'] = labelEncoder.fit_transform(wordlist['label'])\n",
    "df = pd.concat([df, wordlist], ignore_index = False)\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, TRAIN_BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, VALID_BATCH_SIZE)\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=3)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    result.write(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    result.write(\"\\n\")\n",
    "    result.write('-' * 10)\n",
    "    result.write(\"\\n\")\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train_df)\n",
    "    )\n",
    "    result.write(f'Epoch {epoch + 1}/{EPOCHS} | Train loss: {train_loss} | Train accuracy: {train_acc}')\n",
    "    result.write(\"\\n\")\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS} | Train loss: {train_loss} | Train accuracy: {train_acc}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    device,\n",
    "    len(val_df)\n",
    "    )\n",
    "\n",
    "result.write(f'Validation Accuracy : {val_acc}\\n')\n",
    "print(f'Validation Accuracy : {val_acc}\\n')\n",
    "torch.save(model.state_dict(), \"BERT_fine_tuned_accuracy.bin\")\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0a6d5-b723-4cdf-9e2e-f17aab2cb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-Tuning the BERT on the Dataset\n",
    "result = open(\"BERT_Tuninig_results.txt\", \"w\")\n",
    "\n",
    "#=======NOTE: THE FOLLOWING SECTION HAS BEEN CHANGED===============\n",
    "df = pd.read_csv(\"./data/queries_gender_annotated.csv\", names = [\"index\", \"query\", \"label\", \"other\"])\n",
    "df['label'] = df['label'].astype(str)\n",
    "df = df.apply(process_row, axis=1)\n",
    "df.drop(columns=[\"index\", \"other\"], inplace=True)\n",
    "df = df[df['label'].isin(['m', 'n', 'f'])]\n",
    "labelEncoder = LabelEncoder()\n",
    "df['label'] = labelEncoder.fit_transform(df['label'])\n",
    "print(\"Shape of Dataset: {} \\n\".format(df.shape))\n",
    "wordlist = pd.read_csv(\"./data/wordlist_genderspecific.txt\", names = [\"query\", \"label\"])\n",
    "wordlist['label'] = labelEncoder.fit_transform(wordlist['label'])\n",
    "df = pd.concat([df, wordlist], ignore_index = False)\n",
    "result.write(\"Shape of Dataset after concatination with wordlist: {} \\n\".format(df.shape))\n",
    "train_data_loader = create_data_loader(df, tokenizer, MAX_LEN, TRAIN_BATCH_SIZE)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 3) \n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(params =  model.parameters(), lr = LEARNING_RATE, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps = 0,\n",
    "            num_training_steps = total_steps\n",
    "        )\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    result.write(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    result.write(\"\\n\")\n",
    "    result.write('-' * 10)\n",
    "    result.write(\"\\n\")\n",
    "    train_acc, train_loss = train_epoch(\n",
    "                model,\n",
    "                train_data_loader,\n",
    "                optimizer,\n",
    "                device,\n",
    "                scheduler,\n",
    "                len(df)\n",
    "        )\n",
    "    result.write(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    result.write(\"\\n\")\n",
    "\n",
    "torch.save(model.state_dict(), \"BERT_fine_tuned.bin\")\n",
    "result.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0901f5e-d3ac-4e11-864b-58b2bff4f7be",
   "metadata": {},
   "source": [
    "## Author's code for predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8940cea-577c-4b8d-bf1e-cfcbe852af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully\n",
      "Prediction started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/junjianli/anaconda3/anaconda3/envs/myenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Constant variables \n",
    "class_names = ['Female', 'Male' , 'Neutral']\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "TEST_BATCH_SIZE = 16\n",
    "MAX_LEN = 55\n",
    "\n",
    "# Dataset\n",
    "class GenderBiasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, queries, tokenizer, max_len):\n",
    "        self.queries = queries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query_text = str(self.queries[index])\n",
    "         \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            query_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "                'query': query_text,\n",
    "                'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Dataloader\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GenderBiasDataset(\n",
    "    queries = df['query'].to_numpy(),\n",
    "    tokenizer  =tokenizer,\n",
    "    max_len = max_len\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 5 \n",
    "  )\n",
    "\n",
    "#Prediction function\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  query_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      texts = d[\"query\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs[0], dim=1)\n",
    "      query_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs[0])\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  return query_texts, predictions, prediction_probs\n",
    "\n",
    "\n",
    "#Reading MSMarco dev set queries (these queires do not have label)\n",
    "df = pd.read_table(\"./data/msmacro.tsv\") # a dataframe containing the queries CHANGED LINE OF CODE\n",
    "test_data_loader = create_data_loader(df, tokenizer, MAX_LEN, TEST_BATCH_SIZE)\n",
    "\n",
    "#Loading the fine-tuned model - you can download the model from https://drive.google.com/file/d/1_YTRs4v5DVUGUffnRHS_3Yk4qteJKO6w/view?usp=sharing\n",
    "print(\"Loading the Model\")\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 3)\n",
    "model.load_state_dict(torch.load(\"BERT_fine_tuned.bin\", map_location = device))\n",
    "print(\"Model Loaded Successfully\")\n",
    "\n",
    "print(\"Prediction started\")\n",
    "y_query_texts, y_pred, y_pred_probs = get_predictions(model, test_data_loader)\n",
    "prediction = pd.DataFrame(df.values.tolist(), columns = [\"qid\",\"query\"])\n",
    "prediction['female_probability'] = y_pred_probs[:, 0]\n",
    "prediction['male_probability'] = y_pred_probs[:, 1]\n",
    "prediction['neutral_probability'] = y_pred_probs[:, 2]\n",
    "prediction['prediction'] = y_pred\n",
    "prediction.to_csv(\"predictions.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244fbe3-138a-486d-a368-36cdc744d052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bf003-8f93-42cb-a1d2-14532baad711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e577c-af78-4b28-97ef-996017734832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63196552-255d-4f44-b29b-c3bd5556d6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6decea8e-03da-4420-9245-489d3964f9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cec2fd-9a10-4d67-b4e2-6a82c0e7a818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a0bd6-bade-4044-a874-1dbec50375b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063c8b2-cce9-4b70-9cac-7f508d05ec20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca1be5-8f0a-46bf-86ea-ec2dd8499108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8049a7e-ce42-460a-a984-6fafbf6c1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3707, 2)\n",
      "(64, 2)\n",
      "(3771, 3)\n",
      "                                                query  label word\n",
      "0                   who was known as the heretic king      1  NaN\n",
      "1   who plays the main character in night at the m...      2  NaN\n",
      "2                                   what is surrogate      2  NaN\n",
      "3                       how popular is the name katie      0  NaN\n",
      "4          how much sleep in one day does a baby need      2  NaN\n",
      "5          what type of books does karen hesse write?      0  NaN\n",
      "6             can you drink coffee before a mammogram      2  NaN\n",
      "7                   what college did bill gates go to      1  NaN\n",
      "8       who was jacqueline kennedy's social secretary      0  NaN\n",
      "9                                    abbot definition      2  NaN\n",
      "10            concentra how old is a cat in one year.      2  NaN\n",
      "11                   when was stalins response speech      1  NaN\n",
      "12                             who is jennifer garner      0  NaN\n",
      "13              how many weeks can a puppy get spayed      2  NaN\n",
      "14                       who influenced pablo picasso      1  NaN\n",
      "15                 what year did james still graduate      1  NaN\n",
      "16                     actress who born at litrhuania      0  NaN\n",
      "17                                          what cow?      2  NaN\n",
      "18                            susan sarandon cup size      0  NaN\n",
      "19                              who is mark zuckerman      1  NaN\n",
      "20                               who played jake ryan      1  NaN\n",
      "21          who wrote song yesterday when i was young      2  NaN\n",
      "22                        when was actor karthik born      1  NaN\n",
      "23     which prep school did president kennedy attend      1  NaN\n",
      "24                 how much did freddie mercury weigh      1  NaN\n",
      "25                   when was mike tyson's last fight      1  NaN\n",
      "26              what instruments did john denver play      1  NaN\n",
      "27                              who is victoria lomba      0  NaN\n",
      "28  how old should your puppy be to get a rabies shot      2  NaN\n",
      "29                    what is produced during mitosis      2  NaN\n",
      "30                         why did van halen break up      1  NaN\n",
      "31                    who was richard the lionhearted      1  NaN\n",
      "32                        who is king philip of spain      1  NaN\n",
      "33         which microscope were cells first observed      2  NaN\n",
      "34                                  who was herodotus      1  NaN\n",
      "35            how many children does alicia keys have      0  NaN\n",
      "36                   what team did bill russell coach      1  NaN\n",
      "37             what has obama really done for america      1  NaN\n",
      "38                      where is alexander kentucky ?      1  NaN\n",
      "39                                      manual define      2  NaN\n",
      "40                       bruce willis die hard movies      1  NaN\n",
      "41                       who is steve brown comedian?      1  NaN\n",
      "42                                 where is kobenhavn      2  NaN\n",
      "43                did demi moore have plastic surgery      0  NaN\n",
      "44                     age of jennifer lopez pregnant      0  NaN\n",
      "45                              when did lou reed die      1  NaN\n",
      "46                     what happened in cabo shooting      2  NaN\n",
      "47                     distraction medical definition      2  NaN\n",
      "48                          who is richard henry less      1  NaN\n",
      "49           in what year is crepusculario published?      2  NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./data/queries_gender_annotated.csv\", names = [\"index\", \"query\", \"label\", \"other\"])\n",
    "df['label'] = df['label'].astype(str)\n",
    "df = df.apply(process_row, axis=1)\n",
    "df.drop(columns=[\"index\", \"other\"], inplace=True)\n",
    "df = df[df['label'].isin(['m', 'n', 'f'])]\n",
    "print(df.shape)\n",
    "labelEncoder = LabelEncoder()\n",
    "df['label'] = labelEncoder.fit_transform(df['label'])\n",
    "#result.write(\"Shape of Dataset: {} \\n\".format(df.shape))\n",
    "wordlist = pd.read_csv(\"./data/wordlist_genderspecific.txt\", names = [\"word\", \"label\"])\n",
    "wordlist['label'] = labelEncoder.fit_transform(wordlist['label'])\n",
    "print(wordlist.shape)\n",
    "df = pd.concat([df, wordlist], ignore_index = False)\n",
    "#result.write(\"Shape of Dataset after concatination with wordlist: {} \\n\".format(df.shape))\n",
    "print(df.shape)\n",
    "print(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5638569c-e990-4e07-aadb-31aa47823814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     30168\n",
      "query    328609\n",
      "label     30168\n",
      "word     122659\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "434d9aee-c99c-41bb-b265-77f181c1a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3707, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9576), logits=tensor([[-0.2169,  0.0541, -0.1170]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def process_row(row):\n",
    "    if row['label'] not in ['m', 'n', 'f']:\n",
    "        row[\"query\"] = row[\"query\"] + row[\"label\"]\n",
    "        row[\"label\"] = row[\"other\"]\n",
    "    return row\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Minimal Dataset class\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Ensure labels are integers\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load and preprocess a small part of the dataset\n",
    "df = pd.read_csv(\"./data/queries_gender_annotated.csv\", names = [\"index\", \"query\", \"label\", \"other\"])\n",
    "df['label'] = df['label'].astype(str)\n",
    "df = df.apply(process_row, axis=1)\n",
    "df.drop(columns=[\"index\", \"other\"], inplace=True)\n",
    "df = df[df['label'].isin(['m', 'n', 'f'])]\n",
    "print(df.shape)\n",
    "labelEncoder = LabelEncoder()\n",
    "df['label'] = labelEncoder.fit_transform(df['label'])\n",
    "\n",
    "# Encode the data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encodings = tokenizer(df['query'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Ensure labels are integers\n",
    "labels = [int(label) for label in df['label'].tolist()]\n",
    "\n",
    "# Create a minimal dataset and dataloader\n",
    "dataset = SimpleDataset(encodings, labels)\n",
    "data_loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Run a single batch through the model\n",
    "for batch in data_loader:\n",
    "    input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    labels = batch['labels'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    print(outputs)\n",
    "    break  # Only test with the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3986c2-70e4-4442-b879-7124ae64d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1226, 0.0790, 0.4252],\n",
      "        [0.3322, 0.7272, 0.1772],\n",
      "        [0.3759, 0.2836, 0.9362],\n",
      "        [0.6795, 0.5770, 0.6810],\n",
      "        [0.1557, 0.6625, 0.7960]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e992e-fc9e-4780-8bf9-84c5b1fdcd59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7.10",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
